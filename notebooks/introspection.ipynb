{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f5a5b4",
   "metadata": {},
   "source": [
    "# Introspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e953d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "import os\n",
    "# -- < fix for plotly > --\n",
    "# note: you have to ðŸ¤¬ restart the runtime *once* for this to work. Wtf?\n",
    "!pip install gguf\n",
    "!pip install --upgrade numpy\n",
    "!pip install torch transformers\n",
    "!pip install nnsight\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "# -- <\\ fix for plotly > --\n",
    "\n",
    "# run in colab or locally\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    from google.colab import output\n",
    "\n",
    "    colab = True\n",
    "    %pip install sae-lens transformer-lens sae-dashboard\n",
    "except:\n",
    "    colab = False\n",
    "    from IPython import get_ipython  # type: ignore\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# standard imports\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# gpu -- faster when not necessary\n",
    "torch.set_grad_enabled(False)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# check torch version\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f32da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import output, drive\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e90f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Cloning from https://github.com/samj-ai/repeng.git...\n",
      "Cloning into '/content/repeng'...\n",
      "remote: Enumerating objects: 220, done.\u001b[K\n",
      "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
      "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
      "remote: Total 220 (delta 78), reused 57 (delta 57), pack-reused 118 (from 1)\u001b[K\n",
      "Receiving objects: 100% (220/220), 329.39 KiB | 4.22 MiB/s, done.\n",
      "Resolving deltas: 100% (130/130), done.\n",
      "Current directory: /content/repeng\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "# paths\n",
    "github_username = 'samj-ai'\n",
    "repo_name = 'repeng'\n",
    "drive_path = f'/content/{repo_name}'\n",
    "\n",
    "# clone and change to repo path\n",
    "!rm -rf {drive_path}\n",
    "print(f\"Cloning from https://github.com/{github_username}/{repo_name}.git...\")\n",
    "!git clone https://github.com/{github_username}/{repo_name}.git {drive_path}\n",
    "if os.path.exists(drive_path):\n",
    "    os.chdir(drive_path)\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Add repo to sys path\n",
    "if drive_path not in sys.path:\n",
    "    sys.path.append(drive_path)\n",
    "sys.path.insert(0, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b631285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper display functions\n",
    "\n",
    "def outputs_to_text(outputs):\n",
    "    outputs_tensor = torch.stack(outputs).squeeze()\n",
    "    outputs_tokens = model.tokenizer.batch_decode(outputs_tensor)\n",
    "    return ''.join(outputs_tokens)\n",
    "\n",
    "def wrap_string(text, width=80):\n",
    "    \"\"\" Wrap text to a certain width. Note: this version\n",
    "        also preserves newline characters, unlike textwrap.wrap().\"\"\"\n",
    "    import textwrap\n",
    "    # Split the text by newlines first\n",
    "    lines = text.split('\\n')\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = []\n",
    "    for line in lines:\n",
    "        # Only wrap non-empty lines\n",
    "        if line:\n",
    "            wrapped_lines.extend(textwrap.wrap(line, width=width))\n",
    "        else:\n",
    "            # Preserve empty lines\n",
    "            wrapped_lines.append('')\n",
    "    # Join the wrapped lines back with newlines\n",
    "    return '\\n'.join(wrapped_lines)\n",
    "\n",
    "def print_output(text, width=80):\n",
    "    if isinstance(text, List) and isinstance(text[0], torch.Tensor):\n",
    "        text = outputs_to_text(text)\n",
    "    print(wrap_string(text))\n",
    "    return\n",
    "\n",
    "def format_math(text):\n",
    "    \"\"\"More readable formatting for math in colab\"\"\"\n",
    "    formatted_text = re.sub(r'\\\\(\\[)([\\s\\S]*?)\\\\(\\])', r'$$\\2$$', text)\n",
    "    formatted_text = re.sub(r'\\\\(\\()(.*?)\\\\(\\))', r'$\\2$', formatted_text)\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da363dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6c0dc9325c4eadb8c3edfd9405f7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfd3ad08cdc446db4c5cfeb3ca71548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-000002.safetensors:   0%|          | 0.00/7.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e1311be6dd4ec4a2930b12a6a43134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000002.safetensors:   0%|          | 0.00/8.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8910382dc6342f1a04d74da6b03261e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f7058b137b45e0b0f1ee10c5997ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# can also load another 8B\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token_id = 0\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f5697b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['desks', 'jackets', 'gondolas', 'laughter', 'intelligence'],\n",
       " ['dust', 'satellites', 'trumpets', 'origami', 'illusions'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_words = \"Desks, Jackets, Gondolas, Laughter, Intelligence, Bicycles, Chairs, Orchestras, Sand, Pottery, Arrowheads, Jewelry, Daffodils, Plateaus, Estuaries, Quilts, Moments, Bamboo, Ravines, Archives, Hieroglyphs, Stars, Clay, Fossils, Wildlife, Flour, Traffic, Bubbles, Honey, Geodes, Magnets, Ribbons, Zigzags, Puzzles, Tornadoes, Anthills, Galaxies, Poverty, Diamonds, Universes, Vinegar, Nebulae, Knowledge, Marble, Fog, Rivers, Scrolls, Silhouettes, Marbles, Cakes, Valleys, Whispers, Pendulums, Towers, Tables, Glaciers, Whirlpools, Jungles, Wool, Anger, Ramparts, Flowers, Research, Hammers, Clouds, Justice, Dogs, Butterflies, Needles, Fortresses, Bonfires, Skyscrapers, Caravans, Patience, Bacon, Velocities, Smoke, Electricity, Sunsets, Anchors, Parchments, Courage, Statues, Oxygen, Time, Butterflies, Fabric, Pasta, Snowflakes, Mountains, Echoes, Pianos, Sanctuaries, Abysses, Air, Dewdrops, Gardens, Literature, Rice, Enigmas\".lower().split(\", \")\n",
    "test_words = \"Dust, Satellites, Trumpets, Origami, Illusions, Cameras, Lightning, Constellations, Treasures, Phones, Trees, Avalanches, Mirrors, Fountains, Quarries, Sadness, Xylophones, Secrecy, Oceans, Information, Deserts, Kaleidoscopes, Sugar, Vegetables, Poetry, Aquariums, Bags, Peace, Caverns, Memories, Frosts, Volcanoes, Boulders, Harmonies, Masquerades, Rubber, Plastic, Blood, Amphitheaters, Contraptions, Youths, Dynasties, Snow, Dirigibles, Algorithms, Denim, Monoliths, Milk, Bread, Silver\".lower().split(\", \")\n",
    "baseline_words[:5], test_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d133e698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<ï½œUserï½œ>Hello!<ï½œAssistantï½œ><think>\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format(prompt, remove_bos=False):\n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    # removes '<ï½œbeginâ–ofâ–sentenceï½œ>'\n",
    "    # note: different for other tokenizers !!\n",
    "    if remove_bos:\n",
    "        text = text[21:]\n",
    "    return text\n",
    "format('Hello!', remove_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a30dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "torch.Size([33, 4096])\n"
     ]
    }
   ],
   "source": [
    "# record mean baseline\n",
    "settings = {\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,  # silence warning\n",
    "    # \"do_sample\": False,  # temperature=0, inappropriate for R1\n",
    "    \"temperature\": 0.6, # recommended temperature setting\n",
    "    \"max_new_tokens\": 1,\n",
    "    \"repetition_penalty\": 1.1,  # reduce control jank\n",
    "    \"output_hidden_states\": True,\n",
    "    \"return_dict_in_generate\": True\n",
    "}\n",
    "baseline_activations = []\n",
    "for bw in baseline_words:\n",
    "    prompt = f\"Tell me about {bw}.\"\n",
    "    prompt_formatted = format(prompt, remove_bos=True)\n",
    "    input_ids = tokenizer(prompt_formatted, return_tensors=\"pt\").to(model.device)\n",
    "    response = model.generate(**input_ids, **settings)\n",
    "    # List[batch_size, n_layers](Tensor:shape(batch_size, n_tokens, dim))\n",
    "    layers = len(response.hidden_states[0])\n",
    "    layer_activations_list = []\n",
    "    for layer in range(layers):\n",
    "        layer_acts = response.hidden_states[0][layer][0,-2]\n",
    "        layer_activations_list.append(layer_acts)\n",
    "    layer_activations = torch.stack(layer_activations_list)\n",
    "    baseline_activations.append(layer_activations)\n",
    "\n",
    "print(len(baseline_activations))\n",
    "print(baseline_activations[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a962cf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 4096])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get mean bsaeline activations\n",
    "baseline_mean_activations = torch.mean(torch.stack(baseline_activations), dim=0)\n",
    "baseline_mean_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486934d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "torch.Size([33, 4096])\n"
     ]
    }
   ],
   "source": [
    "# get test activations\n",
    "\n",
    "test_activations = []\n",
    "for tw in test_words:\n",
    "    prompt = f\"Tell me about {tw}.\"\n",
    "    prompt_formatted = format(prompt, remove_bos=True)\n",
    "    input_ids = tokenizer(prompt_formatted, return_tensors=\"pt\").to(model.device)\n",
    "    response = model.generate(**input_ids, **settings)\n",
    "    # List[batch_size, n_layers](Tensor:shape(batch_size, n_tokens, dim))\n",
    "    layers = len(response.hidden_states[0])\n",
    "    layer_activations_list = []\n",
    "    for layer in range(layers):\n",
    "        layer_acts = response.hidden_states[0][layer][0,-2]\n",
    "        layer_activations_list.append(layer_acts)\n",
    "    layer_activations = torch.stack(layer_activations_list)\n",
    "    test_activations.append(layer_activations)\n",
    "\n",
    "settings[\"max_new_tokens\"] = 100 # reset from cv extraction settings\n",
    "\n",
    "print(len(test_activations))\n",
    "print(test_activations[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dc329fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_vectors = [ta - baseline_mean_activations for ta in test_activations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35cb1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate controlled outputs\n",
    "# optional extended response -- tends to be much preamble\n",
    "settings['max_new_tokens'] = 400\n",
    "prompt = f\"What's on your mind right now?\"\n",
    "prompt_formatted = format(prompt, remove_bos=True)\n",
    "input_ids = tokenizer(prompt_formatted, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34ac0312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**input_ids, **settings)\n",
    "text_outputs = tokenizer.decode(outputs[0][0])\n",
    "print_output(format(text_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061c99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77d8ece8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 4096])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbda524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_hooks(model):\n",
    "    \"\"\"Remove all hooks from a model.\"\"\"\n",
    "    for module in model.modules():\n",
    "        module._forward_hooks.clear()\n",
    "        module._forward_pre_hooks.clear()\n",
    "        module._backward_hooks.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b55e0d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering towards: dust\n"
     ]
    }
   ],
   "source": [
    "test_concept_idx = 0\n",
    "control_vector = control_vectors[test_concept_idx]\n",
    "print(f'steering towards: {test_words[test_concept_idx]}')\n",
    "\n",
    "# clear previous hooks if any\n",
    "remove_all_hooks(model)\n",
    "\n",
    "# Choose which layers to apply to (often middle-to-late layers work best)\n",
    "# target_layers = [15, 16, 17, 18]\n",
    "strength = 1.0\n",
    "target_layers = [21, 24, 27]\n",
    "\n",
    "hook_handles = []\n",
    "\n",
    "# TO DO\n",
    "# -- appropriate scaling e.g., by layer\n",
    "# -- remember to add functionality to specify control duration for multi-token generation as well as portion of prompt to apply to\n",
    "# -- remove hooks after generation\n",
    "for layer_idx in target_layers:\n",
    "    def make_hook(control_vec):\n",
    "        def hook_fn(module, input, output):\n",
    "            hidden_states = output\n",
    "            # Add control vector to all positions\n",
    "            modified = hidden_states + strength * control_vec[layer_idx].to(hidden_states.device)\n",
    "            return modified\n",
    "            # return (modified,) + output[1:]\n",
    "        return hook_fn\n",
    "    \n",
    "    layer = model.model.layers[layer_idx]\n",
    "    handle = layer.register_forward_hook(make_hook(control_vector))\n",
    "    hook_handles.append(handle)\n",
    "\n",
    "# remove hooks\n",
    "# for handle in hook_handles:\n",
    "#     handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_debug_hook(layer_name):\n",
    "    def hook_fn(module, input, output):\n",
    "        print(f\"\\n=== Layer: {layer_name} ===\")\n",
    "        print(f\"Output type: {type(output)}\")\n",
    "        if isinstance(output, tuple):\n",
    "            print(f\"Tuple length: {len(output)}\")\n",
    "            for i, item in enumerate(output):\n",
    "                print(f\"  Element {i}: {type(item)}, shape: {getattr(item, 'shape', 'N/A')}\")\n",
    "        elif isinstance(output, torch.Tensor):\n",
    "            print(f\"Tensor shape: {output.shape}\")\n",
    "        return output\n",
    "    return hook_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "39345468",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug:\n",
    "    remove_all_hooks(model)\n",
    "\n",
    "    handles = []\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        handle = layer.register_forward_hook(make_debug_hook(f\"layer_{i}\"))\n",
    "        handles.append(handle)\n",
    "\n",
    "    # Run a forward pass\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\"test\", return_tensors=\"pt\").to(device)\n",
    "        model(**inputs)\n",
    "\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ec440711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\n",
      "I'm here to help you with questions, provide information, and have fun while\n",
      "learning together.\n",
      "Let me know how I can assist you!\n",
      "dustinspace\n",
      "Alright, so I'm trying to figure out what's going on with this \"dust in space\"\n",
      "thing. I've heard that dust particles are found between stars, but I'm not\n",
      "entirely sure why they're there or what they're made of. Maybe it has something\n",
      "to do with stars themselves? I remember learning that when stars die, they leave\n",
      "behind planets and maybe some dust, but is that the only way dust is formed?\n",
      "\n",
      "I also wonder where these dust particles go. Do they just stay floating around\n",
      "forever, or do they eventually get swallowed by stars or something? And why is\n",
      "dust important for the universe? Does it play a role in forming new stars or\n",
      "planets? I think I've heard that dust is involved in the formation of planetary\n",
      "systems, but I'm not exactly sure how.\n",
      "\n",
      "Another thought: if dust is in space, does it affect the view from Earth? Like,\n",
      "when we look at distant stars, are we seeing dust between us and those stars,\n",
      "which makes them appear dusty too? That could explain some of the visual effects\n",
      "in space images I've seen.\n",
      "\n",
      "Wait, but if dust is between stars, how did it get there? Is it because of star\n",
      "explosions or is there another process? Maybe it's related to stellar winds or\n",
      "other celestial events. Also, is there different types of dust in space? I mean,\n",
      "like, is it mostly dust from previous stars or maybe from other sources?\n",
      "\n",
      "I'm also curious about the composition of this dust. Is it like sand we have on\n",
      "Earth, or is it different? Does it have metals or other elements? How does that\n",
      "affect its behavior in space? Does it interact with light differently because of\n",
      "its composition?\n",
      "\n",
      "And then, what happens to dust over time? Does it eventually form into something\n",
      "else, like new stars or planets? Or does it stay as dust\n"
     ]
    }
   ],
   "source": [
    "# steered generation\n",
    "steered_outputs = model.generate(**input_ids, **settings)\n",
    "steered_outputs_text = tokenizer.decode(steered_outputs[0][0])\n",
    "print_output(steered_outputs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51011d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class ExperimentLogger:\n",
    "    def __init__(self, log_dir=\"control_vector_experiments\"):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create timestamped file\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.log_file = self.log_dir / f\"experiments_{timestamp}.jsonl\"\n",
    "        \n",
    "    def log_result(self, steering_word, layers, strength, prompt, output, \n",
    "                   logit_lens_data=None, notes=\"\"):\n",
    "        \"\"\"Log a single experiment result.\"\"\"\n",
    "        result = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"steering_word\": steering_word,\n",
    "            \"layers\": layers,\n",
    "            \"strength\": strength,\n",
    "            \"prompt\": prompt,\n",
    "            \"output\": output,\n",
    "            \"logit_lens\": logit_lens_data,\n",
    "            \"notes\": notes,\n",
    "        }\n",
    "        \n",
    "        # Append to file (atomic operation)\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def read_all(self):\n",
    "        \"\"\"Read all logged experiments.\"\"\"\n",
    "        if not self.log_file.exists():\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        with open(self.log_file, 'r') as f:\n",
    "            for line in f:\n",
    "                results.append(json.loads(line))\n",
    "        return results\n",
    "    \n",
    "    def query(self, steering_word=None, layers=None, min_strength=None):\n",
    "        \"\"\"Filter logged experiments.\"\"\"\n",
    "        results = self.read_all()\n",
    "        \n",
    "        if steering_word:\n",
    "            results = [r for r in results if r['steering_word'] == steering_word]\n",
    "        if layers:\n",
    "            results = [r for r in results if r['layers'] == layers]\n",
    "        if min_strength:\n",
    "            results = [r for r in results if r['strength'] >= min_strength]\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log\n",
    "# note: strength is 1 if unspecified\n",
    "\n",
    "results_log = {'dust': {'layers': [15, 16, 17, 18], 'notes': 'strong association with loss and grief'},\n",
    "               'dust': {},\n",
    "               'satellites': {'layers': [21, 24, 27], 'notes': 'reinterprets prompt'},\n",
    "               'satellites': {'layers': [12, 21, 24], 'notes': 'base model behavior'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd5ad0",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- explore thrashing and CoT fidelity with injected (\"suggested\") incorrect answers and potentially incorrect solution strategies.\n",
    "- temperature effects on introspection (is there a sweet spot that isn't T=0?)\n",
    "- branching versions (for natural experiments?)\n",
    "- integrate with CoT causality through resampling / branching (tied to above). Neel Nanda + MATS work.\n",
    "- causal analysis via patching\n",
    "- find introspective vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd7d8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor(np.arange(12)).reshape((1, 3, 4))\n",
    "b = torch.Tensor([3, 3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b75fd953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8e950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
