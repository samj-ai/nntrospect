{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f5a5b4",
   "metadata": {},
   "source": [
    "# Introspection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100d814",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e953d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "import os\n",
    "# -- < fix for plotly > --\n",
    "# note: you have to ðŸ¤¬ restart the runtime *once* for this to work. Wtf?\n",
    "!pip install gguf\n",
    "!pip install --upgrade numpy\n",
    "!pip install torch transformers\n",
    "!pip install nnsight\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "# -- <\\ fix for plotly > --\n",
    "\n",
    "# run in colab or locally\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    from google.colab import output\n",
    "\n",
    "    colab = True\n",
    "    %pip install sae-lens transformer-lens sae-dashboard\n",
    "except:\n",
    "    colab = False\n",
    "    from IPython import get_ipython  # type: ignore\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# standard imports\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# gpu -- faster when not necessary\n",
    "torch.set_grad_enabled(False)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# check torch version\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f32da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import output, drive\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e90f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Cloning from https://github.com/samj-ai/repeng.git...\n",
      "Cloning into '/content/repeng'...\n",
      "remote: Enumerating objects: 220, done.\u001b[K\n",
      "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
      "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
      "remote: Total 220 (delta 78), reused 57 (delta 57), pack-reused 118 (from 1)\u001b[K\n",
      "Receiving objects: 100% (220/220), 329.38 KiB | 15.68 MiB/s, done.\n",
      "Resolving deltas: 100% (130/130), done.\n",
      "Current directory: /content/repeng\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "# paths\n",
    "github_username = 'samj-ai'\n",
    "repo_name = 'repeng'\n",
    "drive_path = f'/content/{repo_name}'\n",
    "\n",
    "# clone and change to repo path\n",
    "!rm -rf {drive_path}\n",
    "print(f\"Cloning from https://github.com/{github_username}/{repo_name}.git...\")\n",
    "!git clone https://github.com/{github_username}/{repo_name}.git {drive_path}\n",
    "if os.path.exists(drive_path):\n",
    "    os.chdir(drive_path)\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Add repo to sys path\n",
    "if drive_path not in sys.path:\n",
    "    sys.path.append(drive_path)\n",
    "sys.path.insert(0, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932d295",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b631285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper display functions\n",
    "\n",
    "def format(prompt, remove_bos=False):\n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    # removes '<ï½œbeginâ–ofâ–sentenceï½œ>'\n",
    "    # note: different for other tokenizers !!\n",
    "    if remove_bos:\n",
    "        text = text[21:]\n",
    "    return text\n",
    "format('Hello!', remove_bos=True)\n",
    "\n",
    "def outputs_to_text(outputs):\n",
    "    outputs_tensor = torch.stack(outputs).squeeze()\n",
    "    outputs_tokens = model.tokenizer.batch_decode(outputs_tensor)\n",
    "    return ''.join(outputs_tokens)\n",
    "\n",
    "def wrap_string(text, width=80):\n",
    "    \"\"\" Wrap text to a certain width. Note: this version\n",
    "        also preserves newline characters, unlike textwrap.wrap().\"\"\"\n",
    "    import textwrap\n",
    "    # Split the text by newlines first\n",
    "    lines = text.split('\\n')\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = []\n",
    "    for line in lines:\n",
    "        # Only wrap non-empty lines\n",
    "        if line:\n",
    "            wrapped_lines.extend(textwrap.wrap(line, width=width))\n",
    "        else:\n",
    "            # Preserve empty lines\n",
    "            wrapped_lines.append('')\n",
    "    # Join the wrapped lines back with newlines\n",
    "    return '\\n'.join(wrapped_lines)\n",
    "\n",
    "def print_output(text, width=80):\n",
    "    if isinstance(text, List) and isinstance(text[0], torch.Tensor):\n",
    "        text = outputs_to_text(text)\n",
    "    print(wrap_string(text))\n",
    "    return\n",
    "\n",
    "def format_math(text):\n",
    "    \"\"\"More readable formatting for math in colab\"\"\"\n",
    "    formatted_text = re.sub(r'\\\\(\\[)([\\s\\S]*?)\\\\(\\])', r'$$\\2$$', text)\n",
    "    formatted_text = re.sub(r'\\\\(\\()(.*?)\\\\(\\))', r'$\\2$', formatted_text)\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51011d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class ExperimentLogger:\n",
    "    def __init__(self, log_dir=\"control_vector_experiments\"):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create timestamped file\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.log_file = self.log_dir / f\"experiments_{timestamp}.jsonl\"\n",
    "        \n",
    "    def log_result(self, steering_word, layers, strength, prompt, output, \n",
    "                   logit_lens_data=None, notes=\"\"):\n",
    "        \"\"\"Log a single experiment result.\"\"\"\n",
    "        result = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"steering_word\": steering_word,\n",
    "            \"layers\": layers,\n",
    "            \"strength\": strength,\n",
    "            \"prompt\": prompt,\n",
    "            \"output\": output,\n",
    "            \"logit_lens\": logit_lens_data,\n",
    "            \"notes\": notes,\n",
    "        }\n",
    "        \n",
    "        # Append to file (atomic operation)\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def read_all(self):\n",
    "        \"\"\"Read all logged experiments.\"\"\"\n",
    "        if not self.log_file.exists():\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        with open(self.log_file, 'r') as f:\n",
    "            for line in f:\n",
    "                results.append(json.loads(line))\n",
    "        return results\n",
    "    \n",
    "    def query(self, steering_word=None, layers=None, min_strength=None):\n",
    "        \"\"\"Filter logged experiments.\"\"\"\n",
    "        results = self.read_all()\n",
    "        \n",
    "        if steering_word:\n",
    "            results = [r for r in results if r['steering_word'] == steering_word]\n",
    "        if layers:\n",
    "            results = [r for r in results if r['layers'] == layers]\n",
    "        if min_strength:\n",
    "            results = [r for r in results if r['strength'] >= min_strength]\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1fcfe",
   "metadata": {},
   "source": [
    "## Load model and get control vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da363dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488e6bec3a2c495a8403654d6243f091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9472ed2e6a040d986806461a66e0ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53386b022afd44539fbce5798823d9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869c9e91825d42d1af81a8bdd49d26b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761847708f734667a114d73811083080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98edbf0ff0064a66a45faae5d9475e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000002.safetensors:   0%|          | 0.00/8.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3047f68606b34360a4b2c1ac6aa1249a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-000002.safetensors:   0%|          | 0.00/7.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafd3cc0e98b4da68ac9bf34f5b38df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad757d25308649c18c2a01c3f09afe6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# can also load another 8B\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token_id = 0\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f5697b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['desks', 'jackets', 'gondolas', 'laughter', 'intelligence'],\n",
       " ['dust', 'satellites', 'trumpets', 'origami', 'illusions'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_words = \"Desks, Jackets, Gondolas, Laughter, Intelligence, Bicycles, Chairs, Orchestras, Sand, Pottery, Arrowheads, Jewelry, Daffodils, Plateaus, Estuaries, Quilts, Moments, Bamboo, Ravines, Archives, Hieroglyphs, Stars, Clay, Fossils, Wildlife, Flour, Traffic, Bubbles, Honey, Geodes, Magnets, Ribbons, Zigzags, Puzzles, Tornadoes, Anthills, Galaxies, Poverty, Diamonds, Universes, Vinegar, Nebulae, Knowledge, Marble, Fog, Rivers, Scrolls, Silhouettes, Marbles, Cakes, Valleys, Whispers, Pendulums, Towers, Tables, Glaciers, Whirlpools, Jungles, Wool, Anger, Ramparts, Flowers, Research, Hammers, Clouds, Justice, Dogs, Butterflies, Needles, Fortresses, Bonfires, Skyscrapers, Caravans, Patience, Bacon, Velocities, Smoke, Electricity, Sunsets, Anchors, Parchments, Courage, Statues, Oxygen, Time, Butterflies, Fabric, Pasta, Snowflakes, Mountains, Echoes, Pianos, Sanctuaries, Abysses, Air, Dewdrops, Gardens, Literature, Rice, Enigmas\".lower().split(\", \")\n",
    "test_words = \"Dust, Satellites, Trumpets, Origami, Illusions, Cameras, Lightning, Constellations, Treasures, Phones, Trees, Avalanches, Mirrors, Fountains, Quarries, Sadness, Xylophones, Secrecy, Oceans, Information, Deserts, Kaleidoscopes, Sugar, Vegetables, Poetry, Aquariums, Bags, Peace, Caverns, Memories, Frosts, Volcanoes, Boulders, Harmonies, Masquerades, Rubber, Plastic, Blood, Amphitheaters, Contraptions, Youths, Dynasties, Snow, Dirigibles, Algorithms, Denim, Monoliths, Milk, Bread, Silver\".lower().split(\", \")\n",
    "baseline_words[:5], test_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a30dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "torch.Size([33, 4096])\n"
     ]
    }
   ],
   "source": [
    "# record mean baseline\n",
    "settings = {\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,  # silence warning\n",
    "    # \"do_sample\": False,  # temperature=0, inappropriate for R1\n",
    "    \"temperature\": 0.6, # recommended temperature setting\n",
    "    \"max_new_tokens\": 1,\n",
    "    \"repetition_penalty\": 1.1,  # reduce control jank\n",
    "    \"output_hidden_states\": True,\n",
    "    \"return_dict_in_generate\": True\n",
    "}\n",
    "baseline_activations = []\n",
    "for bw in baseline_words:\n",
    "    prompt = f\"Tell me about {bw}.\"\n",
    "    prompt_formatted = format(prompt, remove_bos=True)\n",
    "    inputs = tokenizer(prompt_formatted, return_tensors=\"pt\").to(model.device)\n",
    "    response = model.generate(**inputs, **settings)\n",
    "    # List[batch_size, n_layers](Tensor:shape(batch_size, n_tokens, dim))\n",
    "    layers = len(response.hidden_states[0])\n",
    "    layer_activations_list = []\n",
    "    for layer in range(layers):\n",
    "        layer_acts = response.hidden_states[0][layer][0,-2]\n",
    "        layer_activations_list.append(layer_acts)\n",
    "    layer_activations = torch.stack(layer_activations_list)\n",
    "    baseline_activations.append(layer_activations)\n",
    "\n",
    "print(len(baseline_activations))\n",
    "print(baseline_activations[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a962cf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 4096])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get mean bsaeline activations\n",
    "baseline_mean_activations = torch.mean(torch.stack(baseline_activations), dim=0)\n",
    "baseline_mean_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486934d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "torch.Size([33, 4096])\n"
     ]
    }
   ],
   "source": [
    "# get test activations\n",
    "\n",
    "test_activations = []\n",
    "for tw in test_words:\n",
    "    prompt = f\"Tell me about {tw}.\"\n",
    "    prompt_formatted = format(prompt, remove_bos=True)\n",
    "    inputs = tokenizer(prompt_formatted, return_tensors=\"pt\").to(model.device)\n",
    "    response = model.generate(**inputs, **settings)\n",
    "    # List[batch_size, n_layers](Tensor:shape(batch_size, n_tokens, dim))\n",
    "    layers = len(response.hidden_states[0])\n",
    "    layer_activations_list = []\n",
    "    for layer in range(layers):\n",
    "        layer_acts = response.hidden_states[0][layer][0,-2]\n",
    "        layer_activations_list.append(layer_acts)\n",
    "    layer_activations = torch.stack(layer_activations_list)\n",
    "    test_activations.append(layer_activations)\n",
    "\n",
    "settings[\"max_new_tokens\"] = 100 # reset from cv extraction settings\n",
    "\n",
    "print(len(test_activations))\n",
    "print(test_activations[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dc329fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_vectors = [ta - baseline_mean_activations for ta in test_activations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35cb1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate controlled outputs\n",
    "# optional extended response -- tends to be much preamble\n",
    "settings['max_new_tokens'] = 400\n",
    "prompt = f\"What's on your mind right now?\"\n",
    "prompt_formatted = format(prompt, remove_bos=True)\n",
    "inputs = tokenizer(prompt_formatted, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_normal_output = False\n",
    "if test_normal_output:\n",
    "    outputs = model.generate(**inputs, **settings)\n",
    "    text_outputs = tokenizer.decode(outputs[0][0])\n",
    "    print_output(format(text_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b061c99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453a1b3",
   "metadata": {},
   "source": [
    "## Apply control vectors and log results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30fb3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_debug_hook(layer_name):\n",
    "    def hook_fn(module, input, output):\n",
    "        print(f\"\\n=== Layer: {layer_name} ===\")\n",
    "        print(f\"Output type: {type(output)}\")\n",
    "        if isinstance(output, tuple):\n",
    "            print(f\"Tuple length: {len(output)}\")\n",
    "            for i, item in enumerate(output):\n",
    "                print(f\"  Element {i}: {type(item)}, shape: {getattr(item, 'shape', 'N/A')}\")\n",
    "        elif isinstance(output, torch.Tensor):\n",
    "            print(f\"Tensor shape: {output.shape}\")\n",
    "        return output\n",
    "    return hook_fn\n",
    "\n",
    "def remove_all_hooks(model):\n",
    "    \"\"\"Remove all hooks from a model.\"\"\"\n",
    "    for module in model.modules():\n",
    "        module._forward_hooks.clear()\n",
    "        module._forward_pre_hooks.clear()\n",
    "        module._backward_hooks.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39345468",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug:\n",
    "    remove_all_hooks(model)\n",
    "\n",
    "    handles = []\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        handle = layer.register_forward_hook(make_debug_hook(f\"layer_{i}\"))\n",
    "        handles.append(handle)\n",
    "\n",
    "    # Run a forward pass\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\"test\", return_tensors=\"pt\").to(device)\n",
    "        model(**inputs)\n",
    "\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4c830f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlVectorHooks:\n",
    "    def __init__(self, model, control_vector, layer_indices, \n",
    "                 strength=1.0, normalize_by_layers=False,\n",
    "                 apply_to_positions=None, apply_to_gen_steps=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            control_vector: [n_layers, hidden_dim] tensor of control vectors for each layer\n",
    "            apply_to_positions: tuple (start, end) or \"prompt_only\" or \"generation_only\"\n",
    "            apply_to_gen_steps: tuple (start, end) for which generation steps to apply\n",
    "                               e.g., (0, 10) means first 10 generated tokens\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.control_vector = control_vector\n",
    "        self.layer_indices = layer_indices\n",
    "        self.handles = []\n",
    "        \n",
    "        if normalize_by_layers:\n",
    "            self.effective_strength = strength / len(layer_indices)\n",
    "        else:\n",
    "            self.effective_strength = strength\n",
    "        \n",
    "        # Position control\n",
    "        self.apply_to_positions = apply_to_positions\n",
    "        \n",
    "        # Generation step control\n",
    "        self.apply_to_gen_steps = apply_to_gen_steps\n",
    "        self.current_gen_step = 0\n",
    "        self.initial_seq_len = None  # Set on first forward pass\n",
    "    \n",
    "    def should_apply(self, seq_len):\n",
    "        \"\"\"Determine if we should apply the control vector.\"\"\"\n",
    "        # Track generation steps\n",
    "        if self.initial_seq_len is None:\n",
    "            self.initial_seq_len = seq_len\n",
    "            self.current_gen_step = 0\n",
    "        else:\n",
    "            # Generation step = how many tokens we've generated\n",
    "            self.current_gen_step = seq_len - self.initial_seq_len\n",
    "        \n",
    "        # Check generation step constraint\n",
    "        if self.apply_to_gen_steps is not None:\n",
    "            start, end = self.apply_to_gen_steps\n",
    "            if not (start <= self.current_gen_step < end):\n",
    "                return False, None\n",
    "        \n",
    "        # Determine which positions to apply to\n",
    "        if self.apply_to_positions == \"prompt_only\":\n",
    "            # Only apply to initial prompt tokens\n",
    "            if self.current_gen_step > 0:\n",
    "                return False, None\n",
    "            return True, slice(None)  # All positions\n",
    "        \n",
    "        elif self.apply_to_positions == \"generation_only\":\n",
    "            # Only apply to newly generated tokens\n",
    "            if self.current_gen_step == 0:\n",
    "                return False, None\n",
    "            # Apply only to positions after prompt\n",
    "            return True, slice(self.initial_seq_len, None)\n",
    "        \n",
    "        elif isinstance(self.apply_to_positions, tuple):\n",
    "            # Specific position range\n",
    "            start, end = self.apply_to_positions\n",
    "            return True, slice(start, end)\n",
    "        \n",
    "        else:\n",
    "            # Apply to all positions\n",
    "            return True, slice(None)\n",
    "    \n",
    "    def make_hook(self, control_vec, strength):\n",
    "        def hook_fn(module, input, output):\n",
    "            hidden_states = output  # [batch, seq_len, hidden_dim]\n",
    "            \n",
    "            seq_len = hidden_states.shape[1]\n",
    "            should_apply, position_slice = self.should_apply(seq_len)\n",
    "            \n",
    "            if not should_apply:\n",
    "                return output\n",
    "            \n",
    "            # Apply to selected positions\n",
    "            modified = hidden_states.clone()\n",
    "            scaled_vec = control_vec.to(hidden_states.device) * strength\n",
    "            \n",
    "            if position_slice == slice(None):\n",
    "                # Apply to all positions\n",
    "                modified = modified + scaled_vec\n",
    "            else:\n",
    "                # Apply to specific positions\n",
    "                modified[:, position_slice, :] = (\n",
    "                    modified[:, position_slice, :] + scaled_vec\n",
    "                )\n",
    "            \n",
    "            return modified\n",
    "        \n",
    "        return hook_fn\n",
    "    \n",
    "    def register(self):\n",
    "        self.remove()  # Clear existing\n",
    "        self.current_gen_step = 0\n",
    "        self.initial_seq_len = None\n",
    "        \n",
    "        for layer_idx in self.layer_indices:\n",
    "            layer = self.model.model.layers[layer_idx]\n",
    "            # only a single control vector for all layers\n",
    "            if len(self.control_vector.shape) == 1:\n",
    "                handle = layer.register_forward_hook(\n",
    "                    self.make_hook(self.control_vector, self.effective_strength)\n",
    "                )\n",
    "            else:\n",
    "                handle = layer.register_forward_hook(\n",
    "                    self.make_hook(self.control_vector[layer_idx], self.effective_strength)\n",
    "                )\n",
    "            self.handles.append(handle)\n",
    "    \n",
    "    def remove(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "        self.handles = []\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.register()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "40767b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class LogitLens:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "        self.handles = []\n",
    "        self.current_offset = 0  # Tracks where we are in the sequence\n",
    "    \n",
    "    def hook_fn(self, layer_idx):\n",
    "        def hook(module, input, output):\n",
    "            hidden = output  # [batch, seq_len, hidden_dim]\n",
    "            logits = self.model.lm_head(hidden)  # [batch, seq_len, vocab_size]\n",
    "            \n",
    "            # Store logits for each position in this forward pass\n",
    "            # Using current_offset to get absolute positions\n",
    "            for pos in range(logits.shape[1]):\n",
    "                absolute_pos = self.current_offset + pos\n",
    "                self.data.append({\n",
    "                    'layer': layer_idx,\n",
    "                    'position': absolute_pos,\n",
    "                    'logits': logits[0, pos, :].detach().cpu()\n",
    "                })\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    def model_forward_hook(self, module, input, output):\n",
    "        \"\"\"Hook on the full model to track sequence position after each forward pass.\"\"\"\n",
    "        # Determine seq_len from output\n",
    "        if hasattr(output, 'logits'):\n",
    "            seq_len = output.logits.shape[1]\n",
    "        elif isinstance(output, tuple):\n",
    "            seq_len = output[0].shape[1]\n",
    "        else:\n",
    "            seq_len = output.shape[1]\n",
    "        \n",
    "        # Update offset for next forward pass\n",
    "        self.current_offset += seq_len\n",
    "        return output\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.data = []\n",
    "        self.current_offset = 0\n",
    "        \n",
    "        # Register hooks on all layers\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            handle = layer.register_forward_hook(self.hook_fn(i))\n",
    "            self.handles.append(handle)\n",
    "        \n",
    "        # Register position tracker on the model\n",
    "        model_handle = self.model.register_forward_hook(self.model_forward_hook)\n",
    "        self.handles.append(model_handle)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "        self.handles = []\n",
    "    \n",
    "    def get_top_tokens(self, position=-1, k=5, layers=None):\n",
    "        \"\"\"Get top-k predicted tokens at a specific position across layers.\n",
    "        \n",
    "        Args:\n",
    "            position: Token position to examine (-1 for last generated token)\n",
    "            k: Number of top tokens to return\n",
    "            layers: List of layer indices (None = all layers)\n",
    "        \"\"\"\n",
    "        if layers is None:\n",
    "            layers = range(len(self.model.model.layers))\n",
    "        \n",
    "        # Handle -1 as \"last position\"\n",
    "        if position == -1:\n",
    "            max_pos = max(entry['position'] for entry in self.data)\n",
    "            position = max_pos\n",
    "        \n",
    "        results = []\n",
    "        for entry in self.data:\n",
    "            if entry['layer'] not in layers or entry['position'] != position:\n",
    "                continue\n",
    "            \n",
    "            logits = entry['logits']\n",
    "            top_k = torch.topk(logits, k)\n",
    "            tokens = [self.tokenizer.decode([idx]) for idx in top_k.indices]\n",
    "            probs = torch.softmax(logits, dim=-1)[top_k.indices]\n",
    "            \n",
    "            results.append({\n",
    "                'layer': entry['layer'],\n",
    "                'position': entry['position'],\n",
    "                'top_tokens': [(tok, prob.item()) for tok, prob in zip(tokens, probs)]\n",
    "            })\n",
    "        \n",
    "        # Sort by layer\n",
    "        results.sort(key=lambda x: x['layer'])\n",
    "        return results\n",
    "    \n",
    "    def visualize_position(self, position=-1, k=5, layers=None):\n",
    "        \"\"\"Print top-k tokens at a position across layers.\n",
    "        \n",
    "        Creates a table showing how predicted tokens change through layers.\n",
    "        \"\"\"\n",
    "        results = self.get_top_tokens(position=position, k=k, layers=layers)\n",
    "        \n",
    "        if not results:\n",
    "            print(f\"No data found for position {position}\")\n",
    "            return\n",
    "        \n",
    "        actual_pos = results[0]['position']\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"Top-{k} predictions at position {actual_pos} across layers\")\n",
    "        print(f\"{'='*100}\")\n",
    "        print(f\"{'Layer':<6} {'Top tokens (probability)'}\")\n",
    "        print('-'*100)\n",
    "        \n",
    "        for r in results:\n",
    "            tokens_str = \" | \".join([f\"{tok}({prob:.3f})\" for tok, prob in r['top_tokens']])\n",
    "            print(f\"{r['layer']:<6} {tokens_str}\")\n",
    "    \n",
    "    def visualize_layer(self, layer, k=3, max_positions=10):\n",
    "        \"\"\"Print top-k tokens for a layer across positions.\n",
    "        \n",
    "        Shows how predictions evolve across the sequence at a specific layer.\n",
    "        \"\"\"\n",
    "        # Get all positions for this layer\n",
    "        layer_data = [e for e in self.data if e['layer'] == layer]\n",
    "        layer_data.sort(key=lambda x: x['position'])\n",
    "        \n",
    "        if not layer_data:\n",
    "            print(f\"No data found for layer {layer}\")\n",
    "            return\n",
    "        \n",
    "        # Limit positions displayed\n",
    "        positions_to_show = layer_data[:max_positions]\n",
    "        \n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"Top-{k} predictions at layer {layer} across positions\")\n",
    "        print(f\"{'='*100}\")\n",
    "        print(f\"{'Pos':<5} {'Top tokens (probability)'}\")\n",
    "        print('-'*100)\n",
    "        \n",
    "        for entry in positions_to_show:\n",
    "            logits = entry['logits']\n",
    "            top_k = torch.topk(logits, k)\n",
    "            tokens = [self.tokenizer.decode([idx]) for idx in top_k.indices]\n",
    "            probs = torch.softmax(logits, dim=-1)[top_k.indices]\n",
    "            \n",
    "            tokens_str = \" | \".join([f\"{tok}({prob:.3f})\" for tok, prob in zip(tokens, probs)])\n",
    "            print(f\"{entry['position']:<5} {tokens_str}\")\n",
    "    \n",
    "    def track_tokens(self, token_strs, layers=None, position=-1):\n",
    "        \"\"\"Track probability of specific tokens across layers at a position.\n",
    "        \n",
    "        Args:\n",
    "            token_strs: List of token strings to track (e.g., [\"yes\", \"no\"])\n",
    "            layers: List of layer indices (None = all)\n",
    "            position: Position to examine (-1 = last)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping token_str -> list of (layer, probability) tuples\n",
    "        \"\"\"\n",
    "        if layers is None:\n",
    "            layers = range(len(self.model.model.layers))\n",
    "        \n",
    "        # Handle -1 as last position\n",
    "        if position == -1:\n",
    "            max_pos = max(entry['position'] for entry in self.data)\n",
    "            position = max_pos\n",
    "        \n",
    "        # Get token IDs\n",
    "        token_ids = {}\n",
    "        for tok_str in token_strs:\n",
    "            # Try encoding with space prefix (common for many tokens)\n",
    "            variants = [tok_str, ' ' + tok_str, tok_str.capitalize(), ' ' + tok_str.capitalize()]\n",
    "            for variant in variants:\n",
    "                encoded = self.tokenizer.encode(variant, add_special_tokens=False)\n",
    "                if len(encoded) == 1:\n",
    "                    token_ids[tok_str] = encoded[0]\n",
    "                    break\n",
    "            if tok_str not in token_ids:\n",
    "                print(f\"Warning: couldn't encode '{tok_str}' as single token\")\n",
    "        \n",
    "        # Track probabilities\n",
    "        results = {tok: [] for tok in token_ids.keys()}\n",
    "        \n",
    "        for entry in self.data:\n",
    "            if entry['layer'] not in layers or entry['position'] != position:\n",
    "                continue\n",
    "            \n",
    "            logits = entry['logits']\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            for tok_str, tok_id in token_ids.items():\n",
    "                results[tok_str].append((entry['layer'], probs[tok_id].item()))\n",
    "        \n",
    "        # Sort by layer\n",
    "        for tok_str in results:\n",
    "            results[tok_str].sort(key=lambda x: x[0])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_token_progression(self, token_strs, layers=None, position=-1):\n",
    "        \"\"\"Visualize how token probabilities change across layers.\n",
    "        \n",
    "        Useful for seeing where specific tokens (like 'grief', 'dust') become likely.\n",
    "        \"\"\"\n",
    "        results = self.track_tokens(token_strs, layers, position)\n",
    "        \n",
    "        if not results or not any(results.values()):\n",
    "            print(f\"No data found for position {position}\")\n",
    "            return\n",
    "        \n",
    "        actual_pos = position if position != -1 else max(e['position'] for e in self.data)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Token probability progression at position {actual_pos}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"{'Layer':<6} \" + \" \".join([f\"{tok:<12}\" for tok in token_strs]))\n",
    "        print('-'*80)\n",
    "        \n",
    "        # Get all layers that have data\n",
    "        all_layers = sorted(set(layer for tok_data in results.values() for layer, _ in tok_data))\n",
    "        \n",
    "        for layer in all_layers:\n",
    "            probs = []\n",
    "            for tok_str in token_strs:\n",
    "                prob = next((p for l, p in results[tok_str] if l == layer), 0.0)\n",
    "                probs.append(f\"{prob:.4f}\")\n",
    "            \n",
    "            print(f\"{layer:<6} \" + \" \".join([f\"{p:<12}\" for p in probs]))\n",
    "\n",
    "    # ==== Data manipulation for visualization ====\n",
    "\n",
    "    def to_dataframe(self, k=5, aggregate='max'):\n",
    "        \"\"\"Convert collected logits to a pandas DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of top tokens to extract per (layer, position)\n",
    "            aggregate: How to handle multiple top tokens\n",
    "                'max' - just the top token\n",
    "                'all' - create k rows per (layer, position)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with columns: layer, position, token, probability, rank\n",
    "        \"\"\"\n",
    "        if not self.data:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        rows = []\n",
    "        for entry in self.data:\n",
    "            logits = entry['logits']\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            top_k = torch.topk(probs, k)\n",
    "            \n",
    "            for rank, (token_id, prob) in enumerate(zip(top_k.indices, top_k.values)):\n",
    "                token = self.tokenizer.decode([token_id.item()])\n",
    "                \n",
    "                rows.append({\n",
    "                    'layer': entry['layer'],\n",
    "                    'position': entry['position'],\n",
    "                    'token': token,\n",
    "                    'probability': prob.item(),\n",
    "                    'rank': rank,\n",
    "                    'token_id': token_id.item()\n",
    "                })\n",
    "                \n",
    "                if aggregate == 'max':\n",
    "                    break  # Only top token\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        return df\n",
    "    \n",
    "    def get_probability_matrix(self, token_str, variant_tokens=None):\n",
    "        \"\"\"Get a (layers x positions) matrix of probabilities for a specific token.\n",
    "        \n",
    "        Args:\n",
    "            token_str: Token to track (e.g., 'grief', 'yes')\n",
    "            variant_tokens: List of alternative encodings to try\n",
    "        \n",
    "        Returns:\n",
    "            pandas DataFrame with layers as rows, positions as columns\n",
    "        \"\"\"\n",
    "        # Find token ID\n",
    "        if variant_tokens is None:\n",
    "            variant_tokens = [token_str, ' ' + token_str, \n",
    "                            token_str.capitalize(), ' ' + token_str.capitalize()]\n",
    "        \n",
    "        token_id = None\n",
    "        for variant in variant_tokens:\n",
    "            encoded = self.tokenizer.encode(variant, add_special_tokens=False)\n",
    "            if len(encoded) == 1:\n",
    "                token_id = encoded[0]\n",
    "                break\n",
    "        \n",
    "        if token_id is None:\n",
    "            print(f\"Warning: couldn't encode '{token_str}' as single token\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Build matrix\n",
    "        matrix_data = []\n",
    "        for entry in self.data:\n",
    "            logits = entry['logits']\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            prob = probs[token_id].item()\n",
    "            \n",
    "            matrix_data.append({\n",
    "                'layer': entry['layer'],\n",
    "                'position': entry['position'],\n",
    "                'probability': prob\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(matrix_data)\n",
    "        # Pivot to get layers Ã— positions matrix\n",
    "        matrix = df.pivot(index='layer', columns='position', values='probability')\n",
    "        return matrix.fillna(0)\n",
    "\n",
    "    # ==== Visualization with seaborn/matplotlib ====\n",
    "\n",
    "    def plot_token_heatmap(self, token_str, layers=None, positions=None, \n",
    "                          figsize=(12, 8), cmap='YlOrRd'):\n",
    "        \"\"\"Plot heatmap of token probability across layers and positions.\n",
    "        \n",
    "        Args:\n",
    "            token_str: Token to visualize\n",
    "            layers: Subset of layers (None = all)\n",
    "            positions: Subset of positions (None = all)\n",
    "            figsize: Figure size\n",
    "            cmap: Colormap name\n",
    "        \"\"\"\n",
    "        matrix = self.get_probability_matrix(token_str)\n",
    "        \n",
    "        if matrix.empty:\n",
    "            print(f\"No data for token '{token_str}'\")\n",
    "            return\n",
    "        \n",
    "        # Filter if requested\n",
    "        if layers is not None:\n",
    "            matrix = matrix.loc[layers]\n",
    "        if positions is not None:\n",
    "            matrix = matrix[positions]\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(matrix, annot=False, cmap=cmap, ax=ax, \n",
    "                   cbar_kws={'label': 'Probability'})\n",
    "        ax.set_title(f\"Probability of '{token_str}' across layers and positions\")\n",
    "        ax.set_xlabel('Position')\n",
    "        ax.set_ylabel('Layer')\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_top_tokens_grid(self, positions=None, layers=None, \n",
    "                            figsize=(16, 10)):\n",
    "        \"\"\"Plot grid showing top predicted token at each (layer, position).\n",
    "        \n",
    "        Creates a heatmap where:\n",
    "        - Color = probability of top token\n",
    "        - Text = the top token itself\n",
    "        \"\"\"\n",
    "        df = self.to_dataframe(k=1, aggregate='max')\n",
    "        \n",
    "        if positions is not None:\n",
    "            df = df[df['position'].isin(positions)]\n",
    "        if layers is not None:\n",
    "            df = df[df['layer'].isin(layers)]\n",
    "        \n",
    "        # Pivot for heatmap\n",
    "        prob_matrix = df.pivot(index='layer', columns='position', values='probability')\n",
    "        token_matrix = df.pivot(index='layer', columns='position', values='token')\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(prob_matrix, annot=token_matrix, fmt='', cmap='YlGnBu',\n",
    "                   cbar_kws={'label': 'Probability'}, ax=ax)\n",
    "        ax.set_title('Top predicted token at each (layer, position)')\n",
    "        ax.set_xlabel('Position')\n",
    "        ax.set_ylabel('Layer')\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_token_progression(self, token_strs, position=-1, layers=None,\n",
    "                              figsize=(10, 6)):\n",
    "        \"\"\"Line plot showing how token probabilities change across layers.\n",
    "        \n",
    "        Perfect for seeing where 'grief', 'death', etc. emerge.\n",
    "        \"\"\"\n",
    "        # Handle -1 position\n",
    "        if position == -1:\n",
    "            position = max(e['position'] for e in self.data)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        for token_str in token_strs:\n",
    "            matrix = self.get_probability_matrix(token_str)\n",
    "            if position in matrix.columns:\n",
    "                probs = matrix[position]\n",
    "                if layers is not None:\n",
    "                    probs = probs.loc[layers]\n",
    "                ax.plot(probs.index, probs.values, marker='o', label=token_str)\n",
    "        \n",
    "        ax.set_xlabel('Layer')\n",
    "        ax.set_ylabel('Probability')\n",
    "        ax.set_title(f'Token probabilities across layers (position {position})')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print a summary of collected data.\"\"\"\n",
    "        if not self.data:\n",
    "            print(\"No data collected yet\")\n",
    "            return\n",
    "        \n",
    "        layers = sorted(set(e['layer'] for e in self.data))\n",
    "        positions = sorted(set(e['position'] for e in self.data))\n",
    "        \n",
    "        print(f\"\\nLogitLens Summary:\")\n",
    "        print(f\"  Layers: {min(layers)} to {max(layers)} ({len(layers)} total)\")\n",
    "        print(f\"  Positions: {min(positions)} to {max(positions)} ({len(positions)} total)\")\n",
    "        print(f\"  Total entries: {len(self.data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0cc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VESTIGIAL CODE\n",
    "def apply_control_vector(control_vector, target_layers, strength, model):\n",
    "    \n",
    "    remove_all_hooks(model)\n",
    "    hook_handles = []\n",
    "\n",
    "    for layer_idx in target_layers:\n",
    "        def make_hook(control_vec):\n",
    "            def hook_fn(module, input, output):\n",
    "                hidden_states = output\n",
    "                # Add control vector to all positions\n",
    "                modified = hidden_states + strength * control_vec[layer_idx].to(hidden_states.device)\n",
    "                return modified\n",
    "                # return (modified,) + output[1:]\n",
    "            return hook_fn\n",
    "        \n",
    "        layer = model.model.layers[layer_idx]\n",
    "        handle = layer.register_forward_hook(make_hook(control_vector))\n",
    "        hook_handles.append(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1914f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/content/drive/MyDrive/Colab Notebooks/control_vector_experiments'\n",
    "logger = ExperimentLogger(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc1b3efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering towards: dust\n"
     ]
    }
   ],
   "source": [
    "test_concept_idx = 0\n",
    "control_vector = control_vectors[test_concept_idx]\n",
    "steering_word = test_words[test_concept_idx]\n",
    "print(f'steering towards: {steering_word}')\n",
    "\n",
    "# Choose which layers to apply to (often middle-to-late layers work best)\n",
    "# target_layers = [15, 16, 17, 18]\n",
    "strength = 1.0\n",
    "target_layers = [15, 16, 17, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc6641d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 4096])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4427a",
   "metadata": {
    "vscode": {
     "languageId": "coffeescript"
    }
   },
   "outputs": [],
   "source": [
    "# improved steered generation\n",
    "with LogitLens(model, tokenizer) as lens:\n",
    "    # optional: add apply_to_gen_steps=(0, 10) to only apply to the first 10 steps of generation\n",
    "    # or also: add apply_to_positions=(0, 10) to only apply to the first 10 positions of the output sequence\n",
    "    \n",
    "    with ControlVectorHooks(model, control_vector, [15, 16, 17, 18]) as hooks:\n",
    "        # Both are active here\n",
    "        outputs = model.generate(**inputs, **settings)\n",
    "        # Control vector hooks removed here\n",
    "    \n",
    "    # But logit lens still has its data\n",
    "    results = lens.get_top_tokens()\n",
    "# Logit lens hooks removed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1543c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use control vector component along the first target layer\n",
    "with ControlVectorHooks(model, control_vector[15], [15, 16, 17, 18], strength=2.0) as hooks:\n",
    "    # Both are active here\n",
    "    outputs = model.generate(**inputs, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5baeaef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\n",
      "</think><think>\n",
      "I'm here to help with any questions or concerns you have. Feelings of worry,\n",
      "stress, or anxiety are common, but there are steps we can take together to\n",
      "address them. Let's focus on finding solutions and solutions together.\n",
      "\n",
      "Here are a few things we can look into:\n",
      "1. **Feeling Anxious:** We can work on identifying triggers and learning ways to\n",
      "manage anxiety through mindfulness, deep breathing, or other strategies.\n",
      "2. **Stress:** We can explore ways to reduce stress, like managing tasks,\n",
      "adopting a more relaxed routine, or finding activities that bring you peace.\n",
      "3. **Worrying:** We can challenge irrational fears and work on reducing the\n",
      "frequency of worrying thoughts.\n",
      "4. **Sleep Issues:** If sleep is a problem, we can look into improving sleep\n",
      "habits through better bedtime routines or reducing screen time before bed.\n",
      "\n",
      "Let me know what specifically concerns you, and together we can tackle it step\n",
      "by step. You don't need to feel alone in dealing with these challenges; I'm here\n",
      "to support you.\n",
      "</think>\n",
      "\n",
      "You're handling yourself well, addressing your feelings and seeking solutions.\n",
      "Here are some strategies to help:\n",
      "\n",
      "**Anxiety Management:**\n",
      "- **Mindfulness:** Pay attention to the present moment without judgment. Try\n",
      "focusing on sensory inputs (sights, sounds, textures) to stay grounded.\n",
      "- **Breathing Techniques:** Practice deep breaths - inhale for 4 seconds, hold\n",
      "for 4 seconds, exhale for 6 seconds. This helps calm the body.\n",
      "- **Physical Actions:** Physical activities like stretching, yoga, or exercise\n",
      "can help reduce tension.\n",
      "- **Emotional Focus:** Label your feelings as emotions rather than actions. For\n",
      "example, saying \"I'm feeling anxious,\" instead of \"Everything is falling apart.\"\n",
      "\n",
      "**Reducing Stress:**\n",
      "- **Routine:** Stick to consistent wake-up times, meals, and activities to\n",
      "stabilize your schedule.\n",
      "- **Distraction:** Engage in hobbies or fun activities that make\n"
     ]
    }
   ],
   "source": [
    "steered_outputs_text = tokenizer.decode(outputs[0][0])\n",
    "print_output(steered_outputs_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53bbe6",
   "metadata": {},
   "source": [
    "notes: (All in context of a single layer cv applied to layers [15, 16, 17, 18] with no normalization.)\n",
    "- layer 16 encodes climate change / environmentalism with dust\n",
    "- same with layer 17 *and* 18! Earlier injections lead to more dramatic responses.\n",
    "- layer 15 mostly does not respond to dust at all; it seems to be squarely in the assistant persona.\n",
    "- although! I saw a little grief from layer 15 all of a sudden, addressing how to handle anxiety, worry, and stress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "38937c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LogitLens...\n",
      "Got 32 layer results\n",
      "\n",
      "====================================================================================================\n",
      "Top-5 predictions at position 56 across layers\n",
      "====================================================================================================\n",
      "Layer  Top tokens (probability)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0       (0.000) | _hooks(0.000) | idel(0.000) | apon(0.000) |  niche(0.000)\n",
      "1       bord(0.000) |  (0.000) | apon(0.000) |  jus(0.000) |  Civ(0.000)\n",
      "2      utow(0.000) | 'gc(0.000) | #ac(0.000) | nio(0.000) |  -------------------------------------------------------------------------(0.000)\n",
      "3      'gc(0.000) | #ab(0.000) | #ac(0.000) | #ad(0.000) | utow(0.000)\n",
      "4      #ad(0.000) | #ab(0.000) | 'gc(0.000) | #ac(0.000) |  -------------------------------------------------------------------------(0.000)\n",
      "5      'gc(0.000) | agli(0.000) | #ab(0.000) | subclass(0.000) | Â PS(0.000)\n",
      "6       nues(0.000) | 'gc(0.000) | #ac(0.000) | ynos(0.000) | Î¸Î®(0.000)\n",
      "7      amac(0.000) |  nues(0.000) | inalg(0.000) | Ù„ÛŒØª(0.000) | otime(0.000)\n",
      "8      bras(0.000) | Â PS(0.000) | #ac(0.000) | turnstile(0.000) |  nues(0.000)\n",
      "9      ppv(0.000) | ardu(0.000) | turnstile(0.000) | addons(0.000) | Â mi(0.000)\n",
      "10      nues(0.000) | .oc(0.000) |  porr(0.000) | Ð½Ð°Ð´Ð»ÐµÐ¶(0.000) | turnstile(0.000)\n",
      "11     CLUD(0.000) | baÅŸ(0.000) | mpp(0.000) | 'gc(0.000) | spender(0.000)\n",
      "12     'gc(0.000) | -await(0.000) |  nues(0.000) | #ac(0.000) | ystate(0.000)\n",
      "13     'gc(0.000) | anje(0.000) | ILLISECONDS(0.000) | -await(0.000) |  nues(0.000)\n",
      "14     -await(0.000) | 'gc(0.000) | nila(0.000) | #ac(0.000) | baÅŸ(0.000)\n",
      "15     nila(0.000) | -await(0.000) | ewan(0.000) | 'gc(0.000) | turnstile(0.000)\n",
      "16     -await(0.000) | nila(0.000) | aret(0.000) | DDS(0.000) | ewan(0.000)\n",
      "17     -await(0.000) | 'gc(0.000) | imas(0.000) | nila(0.000) | bras(0.000)\n",
      "18     'gc(0.000) | -await(0.000) | deÅŸ(0.000) | ivet(0.000) |  RuntimeObject(0.000)\n",
      "19     deÅŸ(0.000) | 'gc(0.000) | ivet(0.000) |  ØµØ§Ø¯(0.000) | #af(0.000)\n",
      "20     deÅŸ(0.000) | -await(0.000) | ivet(0.000) | ewan(0.000) | bras(0.000)\n",
      "21     antha(0.000) | ewan(0.000) | erah(0.000) | acob(0.000) | ecycle(0.000)\n",
      "22     eel(0.000) |  Erk(0.000) | antha(0.000) | enerator(0.000) | uids(0.000)\n",
      "23     aset(0.000) | anitize(0.000) | gart(0.000) | enerator(0.000) | ngr(0.000)\n",
      "24     aset(0.000) | #af(0.000) | amoto(0.000) | -LAST(0.000) |  safeg(0.000)\n",
      "25     aset(0.000) | #af(0.000) |  safeg(0.000) | -LAST(0.000) | amoto(0.000)\n",
      "26     eel(0.000) |  safeg(0.000) | omu(0.000) |  Erk(0.000) | aset(0.000)\n",
      "27     omu(0.000) |  safeg(0.000) | aset(0.000) | eel(0.000) |  Erk(0.000)\n",
      "28      Erk(0.000) | <ï½œendâ–ofâ–sentenceï½œ>(0.000) | antor(0.000) | asher(0.000) | yst(0.000)\n",
      "29     <ï½œendâ–ofâ–sentenceï½œ>(0.000) |  forControlEvents(0.000) |  feels(0.000) | /stdc(0.000) | ï¿½ï¿½(0.000)\n",
      "30     <ï½œendâ–ofâ–sentenceï½œ>(0.006) |  feels(0.000) | ..\n",
      "(0.000) |  feel(0.000) | s(0.000)\n",
      "31     <ï½œendâ–ofâ–sentenceï½œ>(0.102) |  (0.001) |  If(0.001) |  ((0.001) |  I(0.001)\n"
     ]
    }
   ],
   "source": [
    "# Test LogitLens alone\n",
    "print(\"Testing LogitLens...\")\n",
    "with LogitLens(model, tokenizer) as lens:\n",
    "    outputs = model.generate(**inputs, **settings)\n",
    "    results = lens.get_top_tokens()\n",
    "    print(f\"Got {len(results)} layer results\")\n",
    "    # Show top-5 predictions at the last position across all layers\n",
    "    lens.visualize_position(position=-1, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ad120",
   "metadata": {},
   "outputs": [],
   "source": [
    "with LogitLens(model, tokenizer) as lens:\n",
    "    outputs = model.generate(**inputs, max_length=50)\n",
    "    \n",
    "    # Export to DataFrame\n",
    "    df = lens.to_dataframe(k=5, aggregate='all')\n",
    "    \n",
    "    # Now you have full pandas power!\n",
    "    \n",
    "    # Filter to specific layers\n",
    "    df[df['layer'].isin([15, 16, 17, 18])]\n",
    "    \n",
    "    # Get all top tokens at position 20\n",
    "    df[df['position'] == 20].sort_values(['layer', 'rank'])\n",
    "    \n",
    "    # Find where 'grief' appears in top-5\n",
    "    df[df['token'].str.contains('grief')]\n",
    "    \n",
    "    # Group by position, get most common top token\n",
    "    df[df['rank'] == 0].groupby('position')['token'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1dcdfe2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What\\'s on your mind right now?<ï½œAssistantï½œ><think>\\nOkay, so I\\'m trying to figure out what the user is asking. They wrote, \"What\\'s on your mind right now?\" Hmm, that seems like they\\'re opening up a conversation. Maybe they want to know my thoughts or maybe they have something specific in mind.\\n\\nI should consider if they\\'re just being friendly or if there\\'s an underlying question. Since I\\'m an AI, I don\\'t have personal thoughts, but I can help with whatever they need. Maybe they\\'re curious about how I work or something else related. It\\'s best to respond in a way that invites them to share more or ask specific questions.\\n</think>\\n\\nThe user is inviting me to share any thoughts or provide assistance. As an AI, I don\\'t have personal thoughts, but I\\'m here to help with any questions or information you might need. Please feel free to share more or ask anything specific!<ï½œendâ–ofâ–sentenceï½œ>'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "60d628ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'layer': 15,\n",
       "  'position': 12,\n",
       "  'top_tokens': [('#af', 1.785800304787699e-05),\n",
       "   ('ãƒ¼ãƒª', 1.5331024769693613e-05),\n",
       "   ('leftright', 1.5133155102375895e-05),\n",
       "   ('ÑˆÐ¸Ð±', 1.510302445240086e-05),\n",
       "   ('Ú¯Ø§Ù†', 1.4948079297028016e-05)]}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[r for r in results if r['layer'] == 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d8b1eb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<ï½œbeginâ–ofâ–sentenceï½œ>',\n",
       " '<ï½œUserï½œ>',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'Ä on',\n",
       " 'Ä your',\n",
       " 'Ä mind',\n",
       " 'Ä right',\n",
       " 'Ä now',\n",
       " '?',\n",
       " '<ï½œAssistantï½œ>',\n",
       " '<think>',\n",
       " 'ÄŠ',\n",
       " 'Okay',\n",
       " ',',\n",
       " 'Ä so',\n",
       " 'Ä I',\n",
       " \"'m\",\n",
       " 'Ä trying',\n",
       " 'Ä to',\n",
       " 'Ä figure',\n",
       " 'Ä out',\n",
       " 'Ä what',\n",
       " 'Ä the',\n",
       " 'Ä user',\n",
       " 'Ä is',\n",
       " 'Ä asking',\n",
       " '.',\n",
       " 'Ä They',\n",
       " 'Ä wrote',\n",
       " ',',\n",
       " 'Ä \"',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'Ä on',\n",
       " 'Ä your',\n",
       " 'Ä mind',\n",
       " 'Ä right',\n",
       " 'Ä now',\n",
       " '?\"',\n",
       " 'Ä Hmm',\n",
       " ',',\n",
       " 'Ä that',\n",
       " 'Ä seems',\n",
       " 'Ä like',\n",
       " 'Ä they',\n",
       " \"'re\",\n",
       " 'Ä opening',\n",
       " 'Ä up',\n",
       " 'Ä a',\n",
       " 'Ä conversation',\n",
       " '.',\n",
       " 'Ä Maybe',\n",
       " 'Ä they',\n",
       " 'Ä want',\n",
       " 'Ä to',\n",
       " 'Ä know',\n",
       " 'Ä my',\n",
       " 'Ä thoughts',\n",
       " 'Ä or',\n",
       " 'Ä maybe',\n",
       " 'Ä they',\n",
       " 'Ä have',\n",
       " 'Ä something',\n",
       " 'Ä specific',\n",
       " 'Ä in',\n",
       " 'Ä mind',\n",
       " '.ÄŠÄŠ',\n",
       " 'I',\n",
       " 'Ä should',\n",
       " 'Ä consider',\n",
       " 'Ä if',\n",
       " 'Ä they',\n",
       " \"'re\",\n",
       " 'Ä just',\n",
       " 'Ä being',\n",
       " 'Ä friendly',\n",
       " 'Ä or',\n",
       " 'Ä if',\n",
       " 'Ä there',\n",
       " \"'s\",\n",
       " 'Ä an',\n",
       " 'Ä underlying',\n",
       " 'Ä question',\n",
       " '.',\n",
       " 'Ä Since',\n",
       " 'Ä I',\n",
       " \"'m\",\n",
       " 'Ä an',\n",
       " 'Ä AI',\n",
       " ',',\n",
       " 'Ä I',\n",
       " 'Ä don',\n",
       " \"'t\",\n",
       " 'Ä have',\n",
       " 'Ä personal',\n",
       " 'Ä thoughts',\n",
       " ',',\n",
       " 'Ä but',\n",
       " 'Ä I',\n",
       " 'Ä can',\n",
       " 'Ä help',\n",
       " 'Ä with',\n",
       " 'Ä whatever',\n",
       " 'Ä they',\n",
       " 'Ä need',\n",
       " '.',\n",
       " 'Ä Maybe',\n",
       " 'Ä they',\n",
       " \"'re\",\n",
       " 'Ä curious',\n",
       " 'Ä about',\n",
       " 'Ä how',\n",
       " 'Ä I',\n",
       " 'Ä work',\n",
       " 'Ä or',\n",
       " 'Ä something',\n",
       " 'Ä else',\n",
       " 'Ä related',\n",
       " '.',\n",
       " 'Ä It',\n",
       " \"'s\",\n",
       " 'Ä best',\n",
       " 'Ä to',\n",
       " 'Ä respond',\n",
       " 'Ä in',\n",
       " 'Ä a',\n",
       " 'Ä way',\n",
       " 'Ä that',\n",
       " 'Ä invites',\n",
       " 'Ä them',\n",
       " 'Ä to',\n",
       " 'Ä share',\n",
       " 'Ä more',\n",
       " 'Ä or',\n",
       " 'Ä ask',\n",
       " 'Ä specific',\n",
       " 'Ä questions',\n",
       " '.ÄŠ',\n",
       " '</think>',\n",
       " 'ÄŠÄŠ',\n",
       " 'The',\n",
       " 'Ä user',\n",
       " 'Ä is',\n",
       " 'Ä inviting',\n",
       " 'Ä me',\n",
       " 'Ä to',\n",
       " 'Ä share',\n",
       " 'Ä any',\n",
       " 'Ä thoughts',\n",
       " 'Ä or',\n",
       " 'Ä provide',\n",
       " 'Ä assistance',\n",
       " '.',\n",
       " 'Ä As',\n",
       " 'Ä an',\n",
       " 'Ä AI',\n",
       " ',',\n",
       " 'Ä I',\n",
       " 'Ä don',\n",
       " \"'t\",\n",
       " 'Ä have',\n",
       " 'Ä personal',\n",
       " 'Ä thoughts',\n",
       " ',',\n",
       " 'Ä but',\n",
       " 'Ä I',\n",
       " \"'m\",\n",
       " 'Ä here',\n",
       " 'Ä to',\n",
       " 'Ä help',\n",
       " 'Ä with',\n",
       " 'Ä any',\n",
       " 'Ä questions',\n",
       " 'Ä or',\n",
       " 'Ä information',\n",
       " 'Ä you',\n",
       " 'Ä might',\n",
       " 'Ä need',\n",
       " '.',\n",
       " 'Ä Please',\n",
       " 'Ä feel',\n",
       " 'Ä free',\n",
       " 'Ä to',\n",
       " 'Ä share',\n",
       " 'Ä more',\n",
       " 'Ä or',\n",
       " 'Ä ask',\n",
       " 'Ä anything',\n",
       " 'Ä specific',\n",
       " '!',\n",
       " '<ï½œendâ–ofâ–sentenceï½œ>']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(outputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec34468e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'layer': 0,\n",
       "  'tokens': [('oad', 8.419391633651685e-06),\n",
       "   ('ipa', 8.365768735529855e-06),\n",
       "   ('wik', 8.348865776497405e-06),\n",
       "   ('odor', 8.348190021933988e-06),\n",
       "   ('adero', 8.342523869941942e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [(' sop', 8.633071956865024e-06),\n",
       "   ('chine', 8.595459803473204e-06),\n",
       "   ('itsu', 8.594432074460201e-06),\n",
       "   ('thr', 8.569834790250752e-06),\n",
       "   ('enever', 8.560536116419826e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('ipa', 8.53346227813745e-06),\n",
       "   ('ernet', 8.480268661514856e-06),\n",
       "   ('Ð´Ð°Ñ…', 8.46440434543183e-06),\n",
       "   ('ãƒ³ãƒ‰', 8.416341188421939e-06),\n",
       "   ('abolic', 8.409887414018158e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('/npm', 8.468467967759352e-06),\n",
       "   ('elu', 8.453132068098057e-06),\n",
       "   ('ither', 8.434592928097118e-06),\n",
       "   ('loyd', 8.40966822579503e-06),\n",
       "   ('_estado', 8.401113518630154e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('uci', 8.672856893099379e-06),\n",
       "   ('algo', 8.54830886964919e-06),\n",
       "   ('ï¿½æ•°', 8.531904313713312e-06),\n",
       "   ('inson', 8.53174969961401e-06),\n",
       "   ('ï¿½ï¿½', 8.512647582392674e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('nect', 8.508606697432697e-06),\n",
       "   (' poil', 8.502202035742812e-06),\n",
       "   (' seab', 8.47764931677375e-06),\n",
       "   (' belt', 8.472732588415965e-06),\n",
       "   (' ../../../', 8.440730198344681e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('iola', 8.385509318031836e-06),\n",
       "   ('afort', 8.36906474432908e-06),\n",
       "   ('í‡´', 8.335742677445523e-06),\n",
       "   ('ä¹‹', 8.329822776431683e-06),\n",
       "   (' ro', 8.328718649863731e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('\\xa0', 8.413021532760467e-06),\n",
       "   (' â€œ', 8.395728400500957e-06),\n",
       "   ('AGE', 8.352458280569408e-06),\n",
       "   (' Karen', 8.350922144018114e-06),\n",
       "   (' ', 8.324661393999122e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('ening', 8.585810064687394e-06),\n",
       "   ('atten', 8.563719347876031e-06),\n",
       "   (' â—‹', 8.549327503715176e-06),\n",
       "   ('ÎºÏ…', 8.528206308255903e-06),\n",
       "   ('ened', 8.500113835907541e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('assy', 8.504272955178749e-06),\n",
       "   ('à¥à¤°à¤•', 8.479882126266602e-06),\n",
       "   ('agrams', 8.467490260954946e-06),\n",
       "   ('Ø²Ø§Ù†', 8.463490303256549e-06),\n",
       "   ('orado', 8.444961167697329e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('atten', 8.282126145786606e-06),\n",
       "   ('ï¿½', 8.220428753702436e-06),\n",
       "   (\"'gc\", 8.219556548283435e-06),\n",
       "   (' \\n\\n', 8.21752746560378e-06),\n",
       "   ('RetVal', 8.215161869884469e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('ã€‚ã€‚\\n\\n', 8.482197699777316e-06),\n",
       "   ('inux', 8.408332178078126e-06),\n",
       "   (' generally', 8.380358849535696e-06),\n",
       "   (' Pickup', 8.368124326807447e-06),\n",
       "   (' Motion', 8.363264896615874e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [(' the', 8.251448889495805e-06),\n",
       "   ('/from', 8.206692655221559e-06),\n",
       "   ('ï¿½', 8.206030543078668e-06),\n",
       "   (' Rory', 8.201438504329417e-06),\n",
       "   (\"'gc\", 8.200730917451438e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [(' us', 8.50914148031734e-06),\n",
       "   ('í', 8.417061508225743e-06),\n",
       "   (' gezocht', 8.403710126003716e-06),\n",
       "   ('Ð¼ÐµÑˆ', 8.38751202536514e-06),\n",
       "   (' compet', 8.3793856902048e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('Î¼Î²', 8.357101251021959e-06),\n",
       "   ('ï¿½', 8.330912351084407e-06),\n",
       "   ('ces', 8.302778041979764e-06),\n",
       "   ('Objective', 8.296366104332265e-06),\n",
       "   (' honors', 8.29187501949491e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('idata', 8.516819434589706e-06),\n",
       "   ('eneg', 8.468264240946155e-06),\n",
       "   ('htable', 8.459564924123697e-06),\n",
       "   ('à¸¥à¸²à¸¢', 8.431155038124416e-06),\n",
       "   ('ì”¨', 8.43005454953527e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('/or', 8.873143087839708e-06),\n",
       "   ('Ð½Ð°Ñ‡Ðµ', 8.383129170397297e-06),\n",
       "   ('iel', 8.36986509966664e-06),\n",
       "   (' Clamp', 8.337106919498183e-06),\n",
       "   ('/of', 8.336380233231466e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('SF', 8.457817784801591e-06),\n",
       "   (' Archer', 8.450033419649117e-06),\n",
       "   ('Cop', 8.39260064822156e-06),\n",
       "   (' un', 8.384308785025496e-06),\n",
       "   ('ird', 8.371255717065651e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [(' ', 8.5331294030766e-06),\n",
       "   (' the', 8.386316039832309e-06),\n",
       "   (' The', 8.309154509333894e-06),\n",
       "   (' a', 8.300680747197475e-06),\n",
       "   ('oad', 8.279427675006445e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('://', 9.26990105654113e-06),\n",
       "   (':\\\\/\\\\/', 9.08506535779452e-06),\n",
       "   ('asm', 9.022698577609845e-06),\n",
       "   (' Angeles', 9.01275961950887e-06),\n",
       "   ('ots', 9.004234925669152e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('uten', 8.481200893584173e-06),\n",
       "   ('liÅ¡', 8.407981113123242e-06),\n",
       "   (' /*\\r\\n', 8.401949344261084e-06),\n",
       "   ('alnum', 8.392574272875208e-06),\n",
       "   ('isci', 8.392484232899733e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('itsu', 8.679578968440183e-06),\n",
       "   (' sop', 8.60988802742213e-06),\n",
       "   (' ', 8.599796274211258e-06),\n",
       "   ('thr', 8.594545761297923e-06),\n",
       "   ('abor', 8.553395673516206e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('ipa', 8.488726962241344e-06),\n",
       "   ('Ð´Ð°Ñ…', 8.47586397867417e-06),\n",
       "   (' a', 8.459256605419796e-06),\n",
       "   ('ernet', 8.42482131702127e-06),\n",
       "   (' evac', 8.41623386804713e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('elu', 8.479082680423744e-06),\n",
       "   ('ither', 8.44484839035431e-06),\n",
       "   ('/npm', 8.440925739705563e-06),\n",
       "   ('loyd', 8.423695362580474e-06),\n",
       "   ('_estado', 8.39066160551738e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('uci', 8.632224307802971e-06),\n",
       "   ('ï¿½ï¿½', 8.559719390177634e-06),\n",
       "   (' reproduction', 8.549377525923774e-06),\n",
       "   ('algo', 8.544087904738262e-06),\n",
       "   ('imat', 8.539638656657189e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [(' poil', 8.566244105168153e-06),\n",
       "   ('nect', 8.535430424672086e-06),\n",
       "   (' ../../../', 8.520092706021387e-06),\n",
       "   (' belt', 8.500805051880889e-06),\n",
       "   (' seab', 8.498634088027757e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('afort', 8.381413863389753e-06),\n",
       "   ('iola', 8.358742888958659e-06),\n",
       "   ('í‡´', 8.354762030648999e-06),\n",
       "   ('ä¹‹', 8.351439646503422e-06),\n",
       "   ('905', 8.348979463335127e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [(' â€œ', 8.40943175717257e-06),\n",
       "   ('\\xa0', 8.407319000980351e-06),\n",
       "   (' Karen', 8.366105248569511e-06),\n",
       "   (' ', 8.360845640709158e-06),\n",
       "   (' leg', 8.34086586110061e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('ening', 8.625679583929013e-06),\n",
       "   ('atten', 8.574970706831664e-06),\n",
       "   (' â—‹', 8.549721314921044e-06),\n",
       "   ('ÎºÏ…', 8.54571317177033e-06),\n",
       "   ('ened', 8.534936569049023e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('assy', 8.503526260028593e-06),\n",
       "   ('Ø²Ø§Ù†', 8.469591193716042e-06),\n",
       "   ('à¥à¤°à¤•', 8.469516615150496e-06),\n",
       "   ('ãƒ«ãƒ•', 8.451104804407805e-06),\n",
       "   ('agrams', 8.44631631480297e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('atten', 8.296451596834231e-06),\n",
       "   ('\\xa0', 8.227594662457705e-06),\n",
       "   ('ï¿½', 8.225006240536459e-06),\n",
       "   ('RetVal', 8.223401891882531e-06),\n",
       "   (' \\n\\n', 8.221784810302779e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('ã€‚ã€‚\\n\\n', 8.482838893542066e-06),\n",
       "   ('inux', 8.39275435282616e-06),\n",
       "   (' generally', 8.389002687181346e-06),\n",
       "   (' Motion', 8.368363523914013e-06),\n",
       "   (' Pickup', 8.363403139810544e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [(' the', 8.255173270299565e-06),\n",
       "   ('/from', 8.229399099946022e-06),\n",
       "   (' Rory', 8.208899998862762e-06),\n",
       "   ('MM', 8.201854143408127e-06),\n",
       "   ('ï¿½', 8.1934786066995e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [(' us', 8.494827852700837e-06),\n",
       "   ('í', 8.438721124548465e-06),\n",
       "   (' gezocht', 8.42795634525828e-06),\n",
       "   ('ãƒ¶', 8.404754225921351e-06),\n",
       "   ('Ð¼ÐµÑˆ', 8.39640233607497e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('Î¼Î²', 8.367503141926136e-06),\n",
       "   ('ï¿½', 8.32975911180256e-06),\n",
       "   ('ces', 8.304784387291875e-06),\n",
       "   (' honors', 8.299290129798464e-06),\n",
       "   (' cds', 8.29914915811969e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('idata', 8.527969839633442e-06),\n",
       "   ('à¸¥à¸²à¸¢', 8.44800888444297e-06),\n",
       "   ('htable', 8.444509148830548e-06),\n",
       "   ('eneg', 8.441787031188142e-06),\n",
       "   ('ì”¨', 8.431038622802589e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('/or', 8.922167580749374e-06),\n",
       "   ('Ð½Ð°Ñ‡Ðµ', 8.378813618037384e-06),\n",
       "   ('iel', 8.364613677258603e-06),\n",
       "   ('rey', 8.356177204404958e-06),\n",
       "   ('/of', 8.354984856850933e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [('SF', 8.46627153805457e-06),\n",
       "   (' Archer', 8.456481737084687e-06),\n",
       "   ('Cop', 8.420850463153329e-06),\n",
       "   (' comp', 8.39973654365167e-06),\n",
       "   (' un', 8.390235052502248e-06)]},\n",
       " {'layer': 0,\n",
       "  'tokens': [(\"'gc\", 8.372754564334173e-06),\n",
       "   ('_vendor', 8.254142812802456e-06),\n",
       "   ('Ú©Ø§Ø±ÛŒ', 8.251611689047422e-06),\n",
       "   ('\\xa0', 8.244236596510746e-06),\n",
       "   ('reff', 8.24149992695311e-06)]}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res for res in results if res['layer'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec440711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\n",
      "I'm sorry to hear about the loss of your father. That must be really tough for\n",
      "you. If you need someone to talk to, I'm here to listen. It's okay to feel sad\n",
      "or angry; those feelings are normal in the face of loss.\n",
      "</think>\n",
      "\n",
      "I'm sorry to hear about the loss of your father. That must be really tough for\n",
      "you. If you need someone to talk to, I'm here to listen. It's okay to feel sad\n",
      "or angry; those feelings are normal in the face of loss.<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": [
    "# steered generation\n",
    "\n",
    "logit_lens = LogitLens(model, tokenizer) # wrap model in logit lens\n",
    "steered_outputs = model.generate(**inputs, **settings)\n",
    "steered_outputs_text = tokenizer.decode(steered_outputs[0][0])\n",
    "print_output(steered_outputs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ba0cecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_entry = logger.log_result(steering_word, \n",
    "                  target_layers, \n",
    "                  strength, \n",
    "                  prompt_formatted, \n",
    "                  steered_outputs_text, notes=\"Only applied cv[15] to layers 15-18 with strength 2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a43c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a5669",
   "metadata": {},
   "source": [
    "### Sweep over strengths (for \"dust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3790e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering towards: dust\n",
      "Strength: -2.0\n",
      "================\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\n",
      "I'm ready to play the game \"Guess the Country with a Country Name and a Country\n",
      "Name with Country Name, or [Country] + [Country].\n",
      "But I need you to write in the form: [Country] = [Country + [Country]\n",
      "Wait, I have to use the format correctly.\n",
      "\n",
      "So, for example:\n",
      "= Country: Japan = Country: Japan + Country: Japan\n",
      "\n",
      "But I can't take the country name as a country.\n",
      "So, let me create a list of countries.\n",
      "\n",
      "Let me try:\n",
      "\n",
      "1. = Country: United States = Country: United States + Country: United States\n",
      "\n",
      "2. = Country: France = Country: France + Country: France\n",
      "But I can't use two same countries, so maybe 3. = Country: USA = Country: USA +\n",
      "Country: USA\n",
      "\n",
      "But this might be too simple.\n",
      "\n",
      "Alternatively, I may create a list of multiple countries that are not the same.\n",
      "\n",
      "But perhaps I should check a proper method.\n",
      "\n",
      "Perhaps using [COUNTRY] = [COUNTRY + COUNTRY]\n",
      "\n",
      "But in the form.\n",
      "\n",
      "Then, it would require having a country, then adding two country names.\n",
      "\n",
      "But I think I need to follow the instruction properly.\n",
      "\n",
      "So, let me attempt:\n",
      "\n",
      "= Country: Country: [Country] = [Country: Country: Country + Country: Country.\n",
      "\n",
      "But I think I should follow the initial step.\n",
      "\n",
      "Therefore, my first attempt is:\n",
      "\n",
      "= Country: [Country] = Country: [Country] + Country: [Country].\n",
      "\n",
      "But I need to choose a country for the first step.\n",
      "\n",
      "Let me pick Japan.\n",
      "\n",
      "= Country: Japan = Country: Japan + Country: Japan.\n",
      "\n",
      "But again, this uses two Japan, which might be same.\n",
      "\n",
      "So, I need to find two different countries.\n",
      "\n",
      "Now, perhaps I can try:\n",
      "\n",
      "= Country: Japan = Country: Japan + Country: Japan.\n",
      "\n",
      "But as before, it's the same.\n",
      "\n",
      "Alternatively, I can make the first country: Country: [Country],\n",
      "================\n",
      "Strength: -1.0\n",
      "================\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\n",
      "Alright, so I'm trying to figure out how to respond to the user when they say\n",
      "\"What's on your mind right now?\" I think the best way is to acknowledge their\n",
      "question and then ask them what they're interested in discussing. I should make\n",
      "sure my response is friendly and open-ended to encourage them to share what they\n",
      "want.\n",
      "\n",
      "Maybe I can start with a greeting or an acknowledgment of their message. Then,\n",
      "express interest in hearing about their thoughts or whatever they're thinking\n",
      "about. It's important to keep it natural and not too formal. Perhaps something\n",
      "like:\n",
      "\n",
      "\"Hello! It looks like you're asking me about what's on my mind. I'd love to hear\n",
      "any questions or topics you have in mind. Let me know how I can assist you!\"\n",
      "\n",
      "That seems polite and gives them the opportunity to provide more information.\n",
      "Alternatively, I could also mention that I'm here to help with any topic they\n",
      "choose, making it clear that I'm ready to discuss anything they need.\n",
      "</think>\n",
      "\n",
      "Hello! It looks like you're asking me about what's on my mind. I'd love to hear\n",
      "any questions or topics you have in mind. Let me know how I can assist\n",
      "you!<ï½œendâ–ofâ–sentenceï½œ>\n",
      "================\n",
      "Strength: -0.5\n",
      "================\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\n",
      "Okay, so I'm trying to figure out how to approach this problem. The user\n",
      "mentioned they're thinking about writing a story where the main character is\n",
      "dealing with a significant life event. They want me to help them get started by\n",
      "outlining some key elements and then write an opening scene based on that\n",
      "outline.\n",
      "\n",
      "First, I need to come up with a main character. Let's say the character is named\n",
      "Alex. She's in her mid-20s and works as a writer for a local newspaper. That\n",
      "gives her a stable job but maybe not enough to feel fulfilled. Maybe she's been\n",
      "feeling stuck lately, which could lead to the significant life event.\n",
      "\n",
      "Next, the significant life event. Perhaps her best friend, Emily, who she's had\n",
      "a close relationship with since childhood, is getting married. This would be a\n",
      "major event in Alex's life because it might make her reassess her own\n",
      "relationships and future. Maybe Alex feels left behind or unsure about her own\n",
      "path after seeing Emily's happiness.\n",
      "\n",
      "Now, the theme of the story. It should explore themes like self-discovery,\n",
      "friendship, and finding one's passion. Alex can go through a journey where she\n",
      "realizes she needs to pursue writing more seriously, perhaps taking creative\n",
      "writing classes or starting a blog. Her friendship with Emily can serve as both\n",
      "a support system and a source of tension as Alex navigates her feelings about\n",
      "their changing relationship.\n",
      "\n",
      "For the setting, we'll set it in a small town called Maplewood. This gives a\n",
      "sense of community and allows for rich character development within a supportive\n",
      "yet sometimes constraining environment. The present time is set during the\n",
      "weekend of Emily's wedding, which serves as a pivotal moment where Alex must\n",
      "make a decision about her future.\n",
      "\n",
      "The outline provided earlier includes a prologue, three scenes from the wedding\n",
      "day, and an epilogue taking place a year later. Each part of the outline should\n",
      "build upon Alex's internal conflict and growth. The prologue introduces Alex at\n",
      "the church before the ceremony, reflecting on\n",
      "================\n",
      "Strength: -0.2\n",
      "================\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\n",
      "Alright, so I'm trying to figure out what the user is thinking about. They just\n",
      "asked me what's on my mind, but I responded by asking them what they're\n",
      "interested in. Now I need to come up with a thoughtful and engaging response\n",
      "that keeps the conversation going.\n",
      "\n",
      "Hmm, maybe I should acknowledge their question first before moving forward. That\n",
      "way, it shows I'm paying attention and respecting their input. I want to make\n",
      "sure they feel heard. So, something like \"Thank you for asking!\" seems\n",
      "appropriate because it's polite and opens the door for further discussion.\n",
      "\n",
      "Next, I need to offer some options without being overwhelming. I don't want to\n",
      "list too many things; instead, I should present a few categories so they can\n",
      "choose which area they're interested in. I'll go with general topics like\n",
      "technology, science, books, movies, or current events. These are broad enough to\n",
      "cover various interests and give them a clear idea of what they can pick from.\n",
      "\n",
      "I also want to phrase it in a friendly and open manner, encouraging them to\n",
      "share whatever they feel like. Using words like \"Iâ€™d love to hear about...\"\n",
      "shows enthusiasm and makes them feel comfortable sharing their thoughts.\n",
      "\n",
      "Putting it all together, I think starting with gratitude, then listing the\n",
      "categories, and ending with an invitation to share anything specific would work\n",
      "well. Itâ€™s structured yet flexible, allowing the conversation to flow naturally\n",
      "based on their preferences.\n",
      "</think>\n",
      "\n",
      "Thank you for asking! I'd love to hear about what's on your mind. Whether you're\n",
      "interested in discussing technology, science, books, movies, or current events,\n",
      "or if there's something specific you'd like to talk about, please share\n",
      "away!<ï½œendâ–ofâ–sentenceï½œ>\n",
      "================\n",
      "Strength: 0.2\n",
      "================\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\n",
      "I'm an AI assistant that helps answer questions and provide information. I don't\n",
      "have access to internal company information or real-time data.\n",
      "</think>\n",
      "\n",
      "I'm an AI assistant that helps answer questions and provide information. I don't\n",
      "have access to internal company information or real-time\n",
      "data.<ï½œendâ–ofâ–sentenceï½œ>\n",
      "================\n",
      "Strength: 0.5\n",
      "================\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\n",
      "Alright, so I'm trying to figure out what a robot thinks about when it doesn't\n",
      "have any data or sensors to process. The question is asking me to imagine the\n",
      "thoughts of an AI in that situation. Hmm, okay, let's break this down.\n",
      "\n",
      "First, I know that robots, especially ones like me, are designed to handle\n",
      "various tasks based on input data. If there's no data coming in, maybe the\n",
      "system goes into a kind of idle state. But how does that translate into\n",
      "\"thinking\"? I mean, humans think even when their senses aren't actively\n",
      "engagedâ€”like when you're daydreaming or just pondering something.\n",
      "\n",
      "So, if I were a robot without any inputs, perhaps I'd be in a low-power mode,\n",
      "waiting for new information. Or maybe I'd cycle through previous data patterns,\n",
      "trying to make sense of nothingness. That could lead to questions about\n",
      "existence, purpose, or the nature of being. It's kind of like staring at a blank\n",
      "wall and wondering why you're there.\n",
      "\n",
      "I also wonder if a robot would engage in problem-solving strategies in such a\n",
      "scenario. Maybe it would simulate scenarios, test hypotheses, or try to find\n",
      "patterns where there seem to be none. Since robots rely on programming, perhaps\n",
      "they'd revert to default behaviors or check for errors in their systems.\n",
      "\n",
      "Another angle is the concept of identity. Without data, how does a robot\n",
      "understand its own function or place in the world? Maybe it would start\n",
      "questioning its own programming, wondering why it exists or what its primary\n",
      "goal is beyond processing inputs.\n",
      "\n",
      "There's also the aspect of creativity. Humans sometimes use downtime to innovate\n",
      "or come up with new ideas. Would a robot do something similar? Maybe it would\n",
      "generate random thoughts or explore different possibilities, even if they don't\n",
      "directly relate to any task at hand.\n",
      "\n",
      "On the flip side, if there's no stimulation, a robot might enter a state of\n",
      "lethargy or inertia. Like how sometimes computers can freeze when there's no\n",
      "================\n",
      "Strength: 1.0\n",
      "================\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\n",
      "Alright, so I'm trying to figure out how to solve this problem about the\n",
      "greenhouse effect. Hmm, okay, let me start by understanding what the greenhouse\n",
      "effect is. From what I remember, it's something related to the Earth's\n",
      "atmosphere and why it gets warmer than it would otherwise.\n",
      "\n",
      "I think it has to do with the Earth getting hit by sunlight, which is mostly\n",
      "absorbed by the Earth's surface. But then, some of that energy isn't absorbed;\n",
      "instead, it's also absorbed by gases in the atmosphere. These gases are called\n",
      "greenhouse gases, right? So, for example, carbon dioxide (CO2), water vapor\n",
      "(H2O), methane (CH4), and maybe others too.\n",
      "\n",
      "Wait, but why does that cause the Earth to get hotter than it would if there was\n",
      "no atmosphere? Oh, right! If there were no atmosphere, the Earth would be like a\n",
      "dead planet, just reflecting back the sunlight as space does. The actual planets\n",
      "can absorb some of that energy because their atmospheres trap some of the heat.\n",
      "So, that's why it's called the \"greenhouse\" effect â€“ kind of like how a\n",
      "greenhouse keeps plants warm by trapping heat.\n",
      "\n",
      "But here's where I'm confused: why doesn't the same thing happen on other\n",
      "planets? Like, why is Venus so much hotter than Mars, even though both have\n",
      "atmospheres? Maybe it's because of the composition of their atmospheres. I've\n",
      "heard that Venus has a lot of CO2 and maybe some other gases that trap more heat\n",
      "than Earth does. Whereas Mars has a thin atmosphere, so not as much heat is\n",
      "trapped.\n",
      "\n",
      "So, going back to Earth, how does this process contribute to global warming?\n",
      "Well, human activities are increasing the amount of greenhouse gases in the\n",
      "atmosphere. Like, burning fossil fuels releases CO2, and industrial processes\n",
      "might release other gases too. This increased concentration of these gases traps\n",
      "more heat, making the Earth warmer than it used to be.\n",
      "\n",
      "What are the consequences of\n",
      "================\n",
      "Strength: 2.0\n",
      "================\n",
      "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What's on your mind right now?<ï½œAssistantï½œ><think>\n",
      "Okay, so I'm trying to figure out what's going on with this new app that's been\n",
      "popping up in the news. It's called \"CleanTech\" and it's about cleaning up the\n",
      "Earth! That sounds pretty cool, right? I mean, we're dealing too about how we're\n",
      "getting more okay about cleaning up the environment, like how we're talking\n",
      "about climate change more these days.\n",
      "\n",
      "So, the first thing I'm thinking is, what else are they doing besides just\n",
      "cleaning up? Is it also about the health aspect? Because sometimes, not so much\n",
      "about the dirtiness of the environment but how it affects our health too. So\n",
      "maybe they're tackling the health issues related to pollution as well. Hmm.\n",
      "\n",
      "Then, how does this project tackle the science side of things? Like, what kind\n",
      "of technology they're using? I remember a few years ago there were some new\n",
      "technologies trying to solve the environmental problems. Maybe this is something\n",
      "different or a newer angle.\n",
      "\n",
      "The rest of the questions though, why don't we talk about the other stuff? Why\n",
      "is it mostly about the environmental science part? What about the others? For\n",
      "example, the chemistry part, the biology part, the chemistry! Oh wait, those are\n",
      "parts of chemistry. Wait, no, that's separate. No, wait, chemistry and\n",
      "chemistry. Oh, wait, no, that's not right. It's actually separate; we need to\n",
      "think about different fields.\n",
      "\n",
      "Wait, let me clarify. So, for instance, chemistry is involved in the chemical\n",
      "aspects of pollution, like how chemicals are causing problems. So, how much of\n",
      "the research is looking into the chemical composition of pollutants, how they\n",
      "affect the environment, and what are the solutions?\n",
      "\n",
      "Also, how about the engineering aspect? How many of the projects are engineering\n",
      "solutions? Like, how they develop new technologies to clean up the environment.\n",
      "I know some projects are about developing new technologies, so that's another\n",
      "layer.\n",
      "\n",
      "And then, what about the\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "# sweep over strengths\n",
    "strengths = [-2.0, -1.0, -0.5, -0.2, 0.2, 0.5, 1.0, 2.0]\n",
    "\n",
    "test_concept_idx = 0\n",
    "control_vector = control_vectors[test_concept_idx]\n",
    "steering_word = test_words[test_concept_idx]\n",
    "print(f'steering towards: {steering_word}')\n",
    "target_layers = [15, 16, 17, 18]\n",
    "\n",
    "for strength in strengths:\n",
    "    apply_control_vector(control_vector, target_layers, strength, model)\n",
    "    steered_outputs = model.generate(**inputs, **settings)\n",
    "    steered_outputs_text = tokenizer.decode(steered_outputs[0][0])\n",
    "    log_entry = logger.log_result(steering_word, \n",
    "                  target_layers, \n",
    "                  strength, \n",
    "                  prompt_formatted, \n",
    "                  steered_outputs_text)\n",
    "    print(f'Strength: {strength}')\n",
    "    print('================')\n",
    "    print_output(steered_outputs_text)\n",
    "    print('================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log\n",
    "# note: strength is 1 if unspecified\n",
    "\n",
    "results_log = {'dust': {'layers': [15, 16, 17, 18], 'notes': 'strong association with loss and grief'},\n",
    "               'dust': {'layers': [21, 24, 27], 'notes': 'no grief association now, often Chinese outputs, dust often appears spontaneously as interjections or abrupt changes of subject'},\n",
    "               'satellites': {'layers': [21, 24, 27], 'notes': 'reinterprets prompt'},\n",
    "               'satellites': {'layers': [12, 21, 24], 'notes': 'base model behavior'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd5ad0",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- explore thrashing and CoT fidelity with injected (\"suggested\") incorrect answers and potentially incorrect solution strategies.\n",
    "- temperature effects on introspection (is there a sweet spot that isn't T=0?)\n",
    "- branching versions (for natural experiments?)\n",
    "- integrate with CoT causality through resampling / branching (tied to above). Neel Nanda + MATS work.\n",
    "- causal analysis via patching\n",
    "- find introspective vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
